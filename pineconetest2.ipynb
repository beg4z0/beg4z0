{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "v0to-QXCQjsm"
   },
   "source": [
    "# Retrieval Enhanced Generative Question Answering with OpenAI\n",
    "\n",
    "#### Fixing LLMs that Hallucinate\n",
    "\n",
    "In this notebook we will learn how to query relevant contexts to our queries from Pinecone, and pass these to a generative OpenAI model to generate an answer backed by real data sources. Required installs for this notebook are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpMvHAYRQf9N",
    "outputId": "f2b1a704-1b38-4985-f5cf-be0479f2ac31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU openai pinecone-client datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aEreHNxYkDbK"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# get API key from top-right dropdown on OpenAI website\n",
    "openai.api_key_path = \"api_key.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seS2VDFz0BCI"
   },
   "source": [
    "For many questions *state-of-the-art (SOTA)* LLMs are more than capable of answering correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9FEDn7LvkDYj",
    "outputId": "dea469a8-55ab-491f-f645-356e86d361ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'The 12th person on the moon was Harrison Schmitt, and he landed on December 11, 1972.'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"who was the 12th person on the moon and when did they land?\"\n",
    "\n",
    "# now query text-davinci-003 WITHOUT context\n",
    "res = openai.Completion.create(\n",
    "    engine='text-davinci-003',\n",
    "    prompt=query,\n",
    "    temperature=0,\n",
    "    max_tokens=400,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "res['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfHwX1qSldhY"
   },
   "source": [
    "However, that isn't always the case. Let's first rewrite the above into a simple function so we're not rewriting this every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SczFSfnjmNji"
   },
   "outputs": [],
   "source": [
    "def complete(prompt):\n",
    "    # query text-davinci-003\n",
    "    res = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=400,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return res['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YC6csbA40UW3"
   },
   "source": [
    "Now let's ask a more specific question about training a specific type of transformer model called a *sentence-transformer*. The ideal answer we'd be looking for is _\"Multiple Negatives Ranking (MNR) loss\"_.\n",
    "\n",
    "Don't worry if this is a new term to you, it isn't required to understand what we're doing or demoing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "H2fUC8BtxCt_",
    "outputId": "01beb42c-1f32-4e08-afc5-127e2dc5597a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'If you only have pairs of related sentences, then the best training method to use for sentence transformers is the supervised learning approach. This approach involves providing the model with labeled data, such as pairs of related sentences, and then training the model to learn the relationships between the sentences. This approach is often used for tasks such as natural language inference, semantic similarity, and paraphrase identification.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = (\n",
    "    \"Which training method should I use for sentence transformers when \" +\n",
    "    \"I only have pairs of related sentences?\"\n",
    ")\n",
    "\n",
    "complete(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7ut_DNBwIk1"
   },
   "source": [
    "One of the common answers I get to this is:\n",
    "\n",
    "```\n",
    "The best training method to use for fine-tuning a pre-trained model with sentence transformers is the Masked Language Model (MLM) training. MLM training involves randomly masking some of the words in a sentence and then training the model to predict the masked words. This helps the model to learn the context of the sentence and better understand the relationships between words.\n",
    "```\n",
    "\n",
    "This answer seems pretty convincing right? Yet, it's wrong. MLM is typically used in the pretraining step of a transformer model but *cannot* be used to fine-tune a sentence-transformer, and has nothing to do with having _\"pairs of related sentences\"_.\n",
    "\n",
    "An alternative answer I recieve is about `supervised learning approach` being the most suitable. This is completely true, but it's not specific and doesn't answer the question.\n",
    "\n",
    "We have two options for enabling our LLM in understanding and correctly answering this question:\n",
    "\n",
    "1. We fine-tune the LLM on text data covering the topic mentioned, likely on articles and papers talking about sentence transformers, semantic search training methods, etc.\n",
    "\n",
    "2. We use **R**etrieval **A**ugmented **G**eneration (RAG), a technique that implements an information retrieval component to the generation process. Allowing us to retrieve relevant information and feed this information into the generation model as a *secondary* source of information.\n",
    "\n",
    "We will demonstrate option **2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhWnLkHqmeWI"
   },
   "source": [
    "---\n",
    "\n",
    "## Building a Knowledge Base\n",
    "\n",
    "With open **2** the retrieval of relevant information requires an external _\"Knowledge Base\"_, a place where we can store and use to efficiently retrieve information. We can think of this as the external _long-term memory_ of our LLM.\n",
    "\n",
    "We will need to retrieve information that is semantically related to our queries, to do this we need to use _\"dense vector embeddings\"_. These can be thought of as numerical representations of the *meaning* behind our sentences.\n",
    "\n",
    "There are many options for creating these dense vectors, like open source [sentence transformers](https://pinecone.io/learn/nlp/) or OpenAI's [ada-002 model](https://youtu.be/ocxq84ocYi0). We will use OpenAI's offering in this example.\n",
    "\n",
    "We have already authenticated our OpenAI connection, to create an embedding we just do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EI2iYxq16or9"
   },
   "outputs": [],
   "source": [
    "embed_model = \"text-embedding-ada-002\"\n",
    "\n",
    "res = openai.Embedding.create(\n",
    "    input=[\n",
    "        \"Sample document text goes here\",\n",
    "        \"there will be several phrases in each batch\"\n",
    "    ], engine=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnHpGP5R60Fv"
   },
   "source": [
    "In the response `res` we will find a JSON-like object containing our new embeddings within the `'data'` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57smZFmz61tj",
    "outputId": "30745411-1f44-4abb-ac36-20abcfdbb343"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['object', 'data', 'model', 'usage'])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwSk-wiK62KO"
   },
   "source": [
    "Inside `'data'` we will find two records, one for each of the two sentences we just embedded. Each vector embedding contains `1536` dimensions (the output dimensionality of the `text-embedding-ada-002` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36D4ipOR63AW",
    "outputId": "10a3d6ba-a646-4ebd-d74f-90868d04a6f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPyGLhDX62t4",
    "outputId": "f5d38bb2-f863-4d39-c8f6-d75579634ec9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 1536)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res['data'][0]['embedding']), len(res['data'][1]['embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byxj1rgy68k1"
   },
   "source": [
    "We will apply this same embedding logic to a dataset containing information relevant to our query (and many other queries on the topics of ML and AI).\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "The dataset we will be using is the `doc_article_sections.csv` from our OpenAI demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7uzxkGz73Ov",
    "outputId": "995123eb-8f78-44b0-b325-e0ce2284b168",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"csv\", data_files=\"doc_article_sections.csv\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'title': 'Service Manual G2 74° V6 Service Manual 5010014 Engine Cover And Frame Service [Frame Installation] [Evinrude Service Manual ]',\n 'heading': 'Engine Cover and Frame Service',\n 'content': 'Push  the  air  duct  inward  to  remove  it  from  the center frame. \\n\\n![][142887]\\n1. Air duct\\n\\nRemove  the  tie  strap  from  each  ignition  coil bracket.  \\n\\n![][142888]\\n1. Tie strap\\n2. Fuel injector mount\\n\\n\\n\\nRemove  the  center  frame  (wire  harness  not shown). \\n\\n![][142889]',\n 'tokens': 37}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWWdWjA273qF"
   },
   "source": [
    "The dataset contains many small snippets of text data. We will need to merge many snippets from each video to create more substantial chunks of text that contain more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8b7d062ee1c14bf6b0c55da89ff4b551",
      "8f03d894148346bb90897fb39d6ec686",
      "a115589785f34bc38e1730e8b497eef6",
      "f205f29abe8d47f6b28627684f947bcd",
      "d5aeb124d44744d2aaaa7d5b213caca7",
      "2af9f8bae68d406d8cd4f56acf3db9e4",
      "88f0b5625c9a4ce89d8a30fdf28efd90",
      "a21a6992c8a744d49826ab0f56b867ed",
      "6aa795a589714b058783f5f3eb5983e1",
      "1d70ba5c815a4473939665061e52ae6e",
      "fbd86b292a484498a61acf0ea7f5e814"
     ]
    },
    "id": "uG9ZTI0o-9cJ",
    "outputId": "30b65907-eea0-4de0-c457-d69531e388c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8d2ba4f543a4ca78782be8b58a8fb0d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "new_data = []\n",
    "\n",
    "window = 20  # number of sentences to combine\n",
    "stride = 4  # number of sentences to 'stride' over, used to create overlap\n",
    "\n",
    "for i in tqdm(range(0, len(data), stride)):\n",
    "    i_end = min(len(data)-1, i+window)\n",
    "    if data[i]['title'] != data[i_end]['title']:\n",
    "        # in this case we skip this entry as we have start/end of two videos\n",
    "        continue\n",
    "    slice_data = data[i:i_end]\n",
    "    if all('text' in d for d in slice_data):\n",
    "        text = ' '.join(d['text'] for d in slice_data)\n",
    "        # create the new merged dataset\n",
    "        new_data.append({\n",
    "            'start': data[i]['start'],\n",
    "            'end': data[i_end]['end'],\n",
    "            'title': data[i]['title'],\n",
    "            'text': text,\n",
    "            'id': data[i]['id'],\n",
    "            'url': data[i]['url'],\n",
    "            'published': data[i]['published'],\n",
    "            'channel_id': data[i]['channel_id']\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMyJjt1cnwcH"
   },
   "source": [
    "Now we need a place to store these embeddings and enable a efficient _vector search_ through them all. To do that we use Pinecone, we can get a [free API key](https://app.pinecone.io) and enter it below where we will initialize our connection to Pinecone and create a new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UPNwQTH0RNcl",
    "outputId": "c5d22baf-0e69-4039-fda0-624ce22cd740"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'dimension': 1536,\n 'index_fullness': 0.0,\n 'namespaces': {},\n 'total_vector_count': 0}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone\n",
    "\n",
    "index_name = 'doc-sections'\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone.init(\n",
    "    api_key=\"87c4be87-17d3-4dec-96f1-c20e73fa81e5\",\n",
    "    environment=\"us-east1-gcp\"  # may be different, check at app.pinecone.io\n",
    ")\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=len(res['data'][0]['embedding']),\n",
    "        metric='cosine',\n",
    "        metadata_config={'indexed': ['channel_id', 'published']}\n",
    "    )\n",
    "# connect to index\n",
    "index = pinecone.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nELBmqxxzeqL"
   },
   "source": [
    "We can see the index is currently empty with a `total_vector_count` of `0`. We can begin populating it with OpenAI `text-embedding-ada-002` built embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "bb392ce2d1e047daa1747c4a0f5e89b7",
      "c298c3dc46ed4f2e85e34a9972b3faf4",
      "b335ce0994e045df8a886ca32e3ebb76",
      "b3f2dde1b97c4989b0e6e5ea3365a270",
      "1cbbf96f9b7f46c29ebbd696e5777e82",
      "3172ad39260d41aea64fba5df2c13961",
      "7c20e179ec504d4caed56e17e3f53e02",
      "1a7b9a94c88a4496a24d4e57d4801047",
      "2b2409a6c2024d57b2b6ddb0e76c7068",
      "d26a5434beda435aa5d62c5c11de2bb8",
      "c56384bfac4c4f5596787f04fd76b86c"
     ]
    },
    "id": "vPb9liovzrc8",
    "outputId": "bb69dbce-c140-49af-840f-2c03dd940e2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34ca1f16a839411b9acd467912a47e81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import datetime\n",
    "from time import sleep\n",
    "\n",
    "batch_size = 100  # how many embeddings we create and insert at once\n",
    "\n",
    "for i in tqdm(range(0, len(new_data), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(len(new_data), i+batch_size)\n",
    "    meta_batch = new_data[i:i_end]\n",
    "    # get ids\n",
    "    ids_batch = [x['id'] for x in meta_batch]\n",
    "    # get texts to encode\n",
    "    texts = [x['text'] for x in meta_batch]\n",
    "    # create embeddings (try-except added to avoid RateLimitError)\n",
    "    try:\n",
    "        res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "    except:\n",
    "        done = False\n",
    "        while not done:\n",
    "            sleep(5)\n",
    "            try:\n",
    "                res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "                done = True\n",
    "            except:\n",
    "                pass\n",
    "    embeds = [record['embedding'] for record in res['data']]\n",
    "    # cleanup metadata\n",
    "    meta_batch = [{\n",
    "        'start': x['start'],\n",
    "        'end': x['end'],\n",
    "        'title': x['title'],\n",
    "        'text': x['text'],\n",
    "        'url': x['url'],\n",
    "        'published': x['published'],\n",
    "        'channel_id': x['channel_id']\n",
    "    } for x in meta_batch]\n",
    "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yiF91IbyGYo"
   },
   "source": [
    "Now we search, for this we need to create a _query vector_ `xq`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LF1U_yZGojRJ"
   },
   "outputs": [],
   "source": [
    "res = openai.Embedding.create(\n",
    "    input=[query],\n",
    "    engine=embed_model\n",
    ")\n",
    "\n",
    "# retrieve from Pinecone\n",
    "xq = res['data'][0]['embedding']\n",
    "\n",
    "# get relevant contexts (including the questions)\n",
    "res = index.query(xq, top_k=2, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GH_DkmsNomww",
    "outputId": "fc84b83b-164d-45a7-9e80-9b11519bf25b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'matches': [], 'namespace': ''}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "92NmGGJ1TKQp"
   },
   "outputs": [],
   "source": [
    "limit = 3750\n",
    "\n",
    "def retrieve(query):\n",
    "    res = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        engine=embed_model\n",
    "    )\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = res['data'][0]['embedding']\n",
    "\n",
    "    # get relevant contexts\n",
    "    res = index.query(xq, top_k=3, include_metadata=True)\n",
    "    contexts = [\n",
    "        x['metadata']['text'] for x in res['matches']\n",
    "    ]\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "LwsZuxiTvU2d",
    "outputId": "7e3acf8b-7356-41bc-8c9e-5405a21153e0"
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'prompt' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnboundLocalError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# first we retrieve relevant items from Pinecone\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m query_with_contexts \u001B[38;5;241m=\u001B[39m \u001B[43mretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m query_with_contexts\n",
      "Cell \u001B[1;32mIn[19], line 41\u001B[0m, in \u001B[0;36mretrieve\u001B[1;34m(query)\u001B[0m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(contexts)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m     36\u001B[0m         prompt \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     37\u001B[0m             prompt_start \u001B[38;5;241m+\u001B[39m\n\u001B[0;32m     38\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m---\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(contexts) \u001B[38;5;241m+\u001B[39m\n\u001B[0;32m     39\u001B[0m             prompt_end\n\u001B[0;32m     40\u001B[0m         )\n\u001B[1;32m---> 41\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mprompt\u001B[49m\n",
      "\u001B[1;31mUnboundLocalError\u001B[0m: local variable 'prompt' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# first we retrieve relevant items from Pinecone\n",
    "query_with_contexts = retrieve(query)\n",
    "query_with_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ioDVGF7lkDQL",
    "outputId": "88bbbd48-89b1-4485-f511-cc5014bf3a5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'package com.example.demo.controller;\\n\\nimport com.example.demo.model.User;\\nimport com.example.demo.service.UserService;\\nimport org.springframework.beans.factory.annotation.Autowired;\\nimport org.springframework.web.bind.annotation.*;\\n\\nimport java.util.List;\\n\\n@RestController\\n@RequestMapping(\"/user\")\\npublic class UserController {\\n\\n    @Autowired\\n    private UserService userService;\\n\\n    @GetMapping(\"/all\")\\n    public List<User> getAllUsers(){\\n        return userService.getAllUsers();\\n    }\\n\\n    @PostMapping(\"/add\")\\n    public void addUser(@RequestBody User user){\\n        userService.addUser(user);\\n    }\\n\\n    @PutMapping(\"/update\")\\n    public void updateUser(@RequestBody User user){\\n        userService.updateUser(user);\\n    }\\n\\n    @DeleteMapping(\"/delete/{id}\")\\n    public void deleteUser(@PathVariable int id){\\n        userService.deleteUser(id);\\n    }\\n\\n}'"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then we complete the context-infused query\n",
    "complete(query_with_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPO36aN8QoPZ"
   },
   "source": [
    "And we get a pretty great answer straight away, specifying to use _multiple-rankings loss_ (also called _multiple negatives ranking loss_)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:52:34) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
